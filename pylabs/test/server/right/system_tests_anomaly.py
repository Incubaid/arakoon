"""
Copyright (2010-2014) INCUBAID BVBA

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""




from .. import system_tests_common as Common
from .. import system_tests_common

from arakoon.ArakoonExceptions import *
import arakoon
import nose.tools as NT
from Compat import X
import logging
import time
import sys
import os

CONFIG = Common.CONFIG
def mount_ram_fs ( node_index ) :

    (mount_target,log_dir,tlf_dir,head_dir) = Common.build_node_dir_names( Common.node_names[node_index] )
    if X.fileExists( mount_target ) :
        Common.stopOne( Common.node_names[node_index] )
        cmd = ["umount", mount_target]
        X.subprocess.check_call ( cmd )
        X.removeDirTree( mount_target )

    X.createDir ( mount_target )

    if not os.path.isdir( mount_target ) :
        raise Exception( "%s is not valid mount target as it is not a directory")

    cmd = ["sudo", "mount", "-t", "tmpfs","-o","size=20m","tmpfs", mount_target]
    (rc,out,err) = X.run(cmd)
    if rc:
        logging.info("out=%s", out)
        logging.info("err = %s", err)
        raise Exception("Mounting failed (rc=%s)" % rc)
    

def setup_3_nodes_ram_fs ( home_dir ):
    cluster = Common._getCluster(Common.cluster_id)
    cluster.remove()

    cluster = Common._getCluster(Common.cluster_id)

    logging.info( "Creating data base dir %s" % home_dir )

    X.createDir ( home_dir )

    try :
        for i in range( len(Common.node_names) ):
            mount_ram_fs ( i )
            nodeName = Common.node_names[i]
            (db_dir,log_dir,tlf_dir,head_dir) = Common.build_node_dir_names( Common.node_names[ i ] )
            cluster.addNode (
                nodeName, Common.node_ips[i],
                clientPort = Common.node_client_base_port + i,
                messagingPort = Common.node_msg_base_port + i,
                logDir = log_dir,
                tlfDir = tlf_dir,
                home = db_dir,
                headDir = head_dir)
            cluster.addLocalNode(nodeName)
            cluster.createDirs(nodeName)

    except Exception as ex:
        teardown_ram_fs( True )
        (a,b,c) = sys.exc_info()
        raise a, b, c

    logging.info( "Changing log level to debug for all nodes" )
    cluster.setLogLevel("debug")
    cluster.setMasterLease(int(CONFIG.lease_duration))

    logging.info( "Creating client config" )
    Common.regenerateClientConfig(Common.cluster_id)

    Common.start_all()

def teardown_ram_fs ( removeDirs ):

    logging.info( "Tearing down" )
    Common.stop_all()

    # Copy over log files
    if not removeDirs:
        destination = X.tmpDir + Common.data_base_dir [1:]
        if os.path.isdir( destination ):
            X.removeDirTree( destination )
        X.createDir(destination)
        X.copyDirTree( system_tests_common.data_base_dir, destination)

    for i in range ( len( Common.node_names ) ):
        Common.destroy_ram_fs( i )

    cluster = Common._getCluster(CONFIG.cluster_id)
    cluster.tearDown()


def fill_disk ( file_to_write ) :
    cmd = ["dd", "if=/dev/zero","of=%s" % file_to_write, "bs=1M"]
    try :
        X.subprocess.call(cmd)
    except Exception, ex:
        logging.error( "Caught exception => %s: %s", ex.__class__.__name__, ex )


def build_iptables_block_rules( tcp_port ) :
    rules = list()
    rules.append( "-A INPUT -p tcp -m tcp --dport %s -m state --state NEW,ESTABLISHED -j DROP " % tcp_port )
    rules.append( "-A OUTPUT -p tcp -m tcp --sport %s -m state --state NEW,ESTABLISHED -j DROP " % tcp_port )
    return rules

def get_current_iptables_rules ():
    """
# Generated by iptables-save v1.4.12 on Wed Dec 11 13:20:53 2013
*filter
:INPUT ACCEPT [3820:329240]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [3820:329240]
COMMIT
# Completed on Wed Dec 11 13:20:53 2013
# Generated by iptables-save v1.4.12 on Wed Dec 11 13:20:53 2013
*mangle
:PREROUTING ACCEPT [6373:3028004]
:INPUT ACCEPT [6370:3027234]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [5752:486981]
:POSTROUTING ACCEPT [5690:460009]
COMMIT
# Completed on Wed Dec 11 13:20:53 2013
# Generated by iptables-save v1.4.12 on Wed Dec 11 13:20:53 2013
*nat
:PREROUTING ACCEPT [5:1544]
:INPUT ACCEPT [2:774]
:OUTPUT ACCEPT [92:5639]
:POSTROUTING ACCEPT [92:5639]
COMMIT
# Completed on Wed Dec 11 13:20:53 2013
    """
    rules_file = '/'.join([X.tmpDir, "iptables-rules-save"])
    cmd = ["sudo","/sbin/iptables-save"]
    blob = X.subprocess.check_output(cmd)
    contents = blob.split("\n")
    chain = "---"
    all = {}
    for line in contents:
        if line.startswith("#"):
            pass
        elif line.startswith("*"):
            chain = line[1:].strip()
            chain_rules = all.get(chain)
            if chain_rules is None:
                chain_rules = []
                all[chain] = chain_rules
        elif line.startswith("COMMIT"):
            chain = "---"
        else:
            rule = line.strip()
            if len(rule) == 0 or rule.startswith(":"):
                pass
            else:
                chain_rules.append(rule)

    print all
    return all

def apply_iptables_rules ( rules ) :

    flush_all_rules()
    for table in rules.keys():
        for line in rules[table]:
            cmd = "sudo /sbin/iptables -t %s %s" %(table,line) 
            logging.info ("cmd = %s", cmd)
            cmd = cmd.strip()
            cmd = cmd.split(' ')
            logging.info("cmd=%s", cmd)
            X.subprocess.check_call(cmd)


def block_tcp_port ( tcp_port ):
    current_rules = get_current_iptables_rules ()
    rules = build_iptables_block_rules( tcp_port )

    for rule in rules :
        chain = current_rules.get("filter")
        if chain is None:
            chain = []
            current_rules["filter"] = chain
        if rule in chain:
            return
        else:
            chain.append(rule)

    apply_iptables_rules( current_rules )

@Common.with_custom_setup(setup_3_nodes_ram_fs,
                          teardown_ram_fs )
def test_disk_full_on_slave ():
    cli = Common.get_client()
    master_id = cli.whoMaster()
    slave_id = Common.node_names[0]
    if slave_id == master_id:
        slave_id = Common.node_names[1]

    logging.info( "Got master '%s' and slave '%s'", master_id, slave_id  )

    disk_full_scenario( slave_id, cli )


def disk_full_scenario( node_id, cli ):
    cluster = Common._getCluster(CONFIG.cluster_id)
    node_home = cluster.getNodeConfig(node_id ) ['home']

    disk_filling = '/'.join([node_home , "filling"])
    disk_filler = lambda: fill_disk ( disk_filling )
    set_loop = lambda: Common.iterate_n_times( 500, Common.simple_set, 500 )
    set_get_delete_loop = lambda: Common.iterate_n_times(10000,
                                                         Common.set_get_and_delete,
                                                         1000 )

    Common.iterate_n_times( 500, Common.simple_set )

    try :
        Common.create_and_wait_for_thread_list ( [ disk_filler,
                                                   set_loop,
                                                   set_get_delete_loop ] )
    except Exception, ex:
        logging.error( "Caught exception when disk was full => %s: %s", ex.__class__.__name__, ex )

    os.remove( disk_filling )
    cli.dropConnections()

    # Make sure the node with a full disk is no longer running
    NT.assert_equals( cluster.getStatusOne(node_id),
                      X.AppStatusType.HALTED,
                      'Node with full disk is still running')
    time.sleep( 2* Common.lease_duration )

    cli2 = Common.get_client()
    cli2.whoMaster()
    cli2.dropConnections()

    Common.start_all()

    time.sleep( Common.lease_duration )

    if node_id == Common.node_names[0] :
        node_2 = Common.node_names[0]
    else :
        node_2 = Common.node_names[1]

    Common.stop_all()
    Common.assert_last_i_in_sync ( node_id, node_2 )
    Common.compare_stores( node_id, node_2 )
    Common.start_all()

    Common.iterate_n_times( 500, Common.set_get_and_delete , 100000 )
    key_list = cli.prefix( "key_", 500 )
    Common.assert_key_list( 0, 500, key_list )


def unblock_node_ports ( node_id ):
    current_rules = get_current_iptables_rules()
    filter = current_rules.get("filter")
    if filter is None:
        filter = {}
    new_rules = []

    blockers = set( get_node_block_rules( node_id ))
    for rule in filter:
        if rule in blockers:
            pass
        else:
            new_rules.append(rule)
    current_rules["filter"] = new_rules
    apply_iptables_rules( current_rules )


def block_node_ports ( node_id ):
    current_rules = get_current_iptables_rules()

    filter = current_rules.get("filter")
    if filter is None:
        filter = {}
    new_rules = []
    blockers = get_node_block_rules( node_id )

    for rule in filter:
        if rule in blockers:
            pass
        else:
            new_rules.append(rule)

    current_rules["filter"] = new_rules
    apply_iptables_rules( current_rules)

def get_node_block_rules( node_id ):
    node_ports = get_node_ports( node_id )
    NT.assert_not_equals( len(node_ports), 0, "Could not determine node ports")
    node_block_rules = list()

    for node_port in node_ports:
        node_block_rules.extend( build_iptables_block_rules( node_port ) )

    return node_block_rules

def get_node_ports ( node_id ):
    cluster = Common._getCluster(Common.cluster_id)
    node_pid = cluster._getPid(node_id)
    cmd = "netstat -natp | grep %s\/arakoon | awk '// {print $4}' | cut -d ':' -f 2 | sort -u" % node_pid
    stdout = X.subprocess.check_output([cmd], shell = True )
    print stdout
    port_list = stdout.strip().split("\n")

    return port_list

def iterate_block_unblock_nodes ( iter_cnt, node_ids, period = 1.0 ):
    for i in range( iter_cnt ) :
        time.sleep( period )
        for node_id in node_ids:
            block_node_ports( node_id )
        time.sleep( period )
        for node_id in node_ids:
            unblock_node_ports( node_id )

def iterate_block_unblock_single_slave ( ):
    cli = get_client()
    master_id = cli.whoMaster()
    cli.dropConnections()

    if ( master_id is None ) :
        logging.error( "Cannot determine who is master skipping this method")
        return
    nns = Common.node_names
    slave_index = ( nns.index( master_id ) + 1 ) % len ( nns )
    slave_id = nns[ slave_index ]
    iterate_block_unblock_nodes ( 60, [ slave_id ] )

def iterate_block_unblock_both_slaves ( ):
    cli = get_client()
    master_id = cli.whoMaster()
    cli.dropConnections()

    if ( master_id is None ) :
        logging.error( "Cannot determine who is master skipping this method")
        return

    slave_list = list()
    for node in Common.node_names :
        if node == master_id :
            continue
        slave_list.append( node )

    iterate_block_unblock_nodes ( 60, slave_list )

def iterate_block_unblock_master ( ):
    cli = get_client()
    master_id = cli.whoMaster()
    cli.dropConnections()

    if ( master_id is None ) :
        logging.error( "Cannot determine who is master skipping this method")
        return

    iterate_block_unblock_nodes ( 60, [ master_id ] )

def flush_all_rules() :
    cmd = ["sudo","/sbin/iptables","-F"]
    X.subprocess.call(cmd)

def iptables_teardown( removeDirs ) :
    flush_all_rules ()
    Common.basic_teardown ( removeDirs )
    logging.info( "iptables teardown complete" )

@Common.with_custom_setup(Common.setup_3_nodes_forced_master,
                          iptables_teardown )
def test_block_single_slave_ports_loop () :
    master_id = Common.node_names [0]
    # Node 0 is fixed master
    slave_id = Common.node_names[1]

    write_loop = lambda: Common.iterate_n_times( 10000,
                                                 Common.set_get_and_delete )
    block_loop = lambda: iterate_block_unblock_nodes ( 10, [ slave_id ] )

    Common.create_and_wait_for_thread_list( [write_loop, block_loop] )

    # Make sure the slave is notified of running behind
    cli = Common.get_client()
    cli.set( 'key', 'value')

    # Give the slave some time to catchup
    time.sleep(5.0)
    Common.flush_store (master_id)
    Common.flush_store (slave_id)
    Common.stop_all()
    Common.assert_last_i_in_sync ( master_id, slave_id )
    Common.compare_stores( master_id, slave_id )

@Common.with_custom_setup(Common.setup_3_nodes,
                          iptables_teardown )
def test_block_master_ports () :

    def validate_reelection( old_master_id) :

        # Leave some time for re-election
        time.sleep( 1.5 * Common.lease_duration )

        cli_cfg_no_master = dict ( cli._config.getNodes() )
        # Kick out old master out of config, he is unaware of who is master
        cli_cfg_no_master.pop(old_master_id)
        new_cli = arakoon.Arakoon.ArakoonClient(
            arakoon.Arakoon.ArakoonClientConfig(CONFIG.cluster_id,
                                                cli_cfg_no_master) )

        new_master_id = new_cli.whoMaster()

        NT.assert_not_equals( new_master_id, None, "No new master elected." )
        new_cli.dropConnections()

        cli_cfg_only_master = dict()
        for key in cli._config.getNodes().keys() :
            if key == old_master_id :
                cli_cfg_only_master [ key ] = cli._config.getNodeLocations( key  )
        new_cli = arakoon.Arakoon.ArakoonClient(
            arakoon.Arakoon.ArakoonClientConfig(CONFIG.cluster_id,
                                                cli_cfg_only_master ) )
        NT.assert_raises( arakoon.ArakoonExceptions.ArakoonNoMaster, new_cli.whoMaster )
        new_cli.dropConnections()

        return new_master_id

    cli = Common.get_client()
    cli.nop()
    old_master_id = cli.whoMaster()

    master_ports = get_node_ports( old_master_id )
    cluster = Common._getCluster(Common.cluster_id)
    master_client_port = cluster.getNodeConfig(old_master_id ) ["client_port"]
    master_ports.remove( master_client_port )
    block_rules = []
    for master_port in master_ports:
        block_rules.extend (build_iptables_block_rules (master_port) )

    rules = {"filter": block_rules}
    apply_iptables_rules(rules )

    NT.assert_raises( X.arakoon_client.ArakoonException, cli.set, "key", "value" )

    new_master_id = validate_reelection( old_master_id )

    flush_all_rules()

    cli._masterId = None
    Common.set_get_and_delete( cli, "k1", "v1")
    cli.dropConnections()
